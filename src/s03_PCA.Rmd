---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---


# load packages
```{r}
rm(list = ls())
library(data.table)
library(reshape2)
library(ggplot2)
library(scico)
library(png)

today = Sys.Date()
```

# load data
```{r}
# simulation parameters
#params = read.table("../config/parameters.tsv", sep = "\t", header = T)
params = read.table("/mnt/scratch/robe1195/Josephs_Lab_Projects/selection-demography-cnn/config/parameters.tsv", sep = "\t", header = T)

# get random sample of simulation outputs
mypics = list.files("../workflow/data/images", full.names = T)

# get sim ids
picids = as.numeric(gsub(".png", "", gsub("../workflow/data/images/slim_", "", mypics)))

# get variable that outputs should predict 
y = params[picids, "sweepS"]
summary(y)

# define function for reading pngs,
read_png = function(x){
  print(x)
  temp_pic = readPNG(x)[,,1]
  melt(temp_pic)$value
}

# load sim outputs
mypics = lapply(mypics, read_png)

# bind all sim outputs together
mypics = do.call("cbind", mypics)

# convert gray pixels to 0.5
mypics[((mypics > 0) & (mypics < 1))] = 0.5

# simulations should be rows
# do PCA
pca_results = as.data.frame(prcomp(t(mypics), center = T)$x)

# visually show how PCs correlate with dependent variable
ggplot(pca_results, aes(PC1, PC2, color = y)) +
  geom_point() +
  theme_classic() +
  scale_color_scico(palette = "lajolla", name = "Selection coefficient")

# test if PC's correlate with selection coefficient
plot(pca_results$PC1, y)
cor.test(pca_results$PC1, y, method = "pearson")

plot(pca_results$PC2, y)
cor.test(pca_results$PC2, y, method = "pearson")

plot(pca_results$PC3, y)
cor.test(pca_results$PC3, y, method = "pearson")
```

# compare predictions to actual values on testing dataset
```{r}
#pred_vs_act = read.table("../workflow/predicted_vs_actual.txt")
pred_vs_act = read.table("/mnt/scratch/robe1195/Josephs_Lab_Projects/selection-demography-cnn/workflow/predicted_vs_actual.txt")

# subset data to remove outliers
#pred_vs_act = pred_vs_act[(pred_vs_act$V1 <= 5000),]

# distribution of fixation times

pred_vs_act_melt = melt(pred_vs_act, id = "V1")
pred_vs_act_melt$variable = as.character(pred_vs_act_melt$variable)
pred_vs_act_melt$variable[(pred_vs_act_melt$variable == "V2")] = "Actual"
pred_vs_act_melt$variable[(pred_vs_act_melt$variable == "V3")] = "Predicted"

ggplot(pred_vs_act_melt, aes(x = value, color = variable)) +
  geom_density() +
  theme_classic() +
  labs(x = "log10(Fixation time, generations)", color = "Type")

# nicer plots
my_cor_coeff = cor.test(pred_vs_act$V2, pred_vs_act$V3, method = "pearson")

cor_est = signif(my_cor_coeff$estimate, digits = 3)
cor_lwb = signif(my_cor_coeff$conf.int[1], digits = 3)
cor_upb = signif(my_cor_coeff$conf.int[2], digits = 3)
cor_pvalue = signif(my_cor_coeff$p.value, digits = 3)

ggplot(aes(x = V2, y = V3), data = pred_vs_act) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "#8C0172", linewidth = 2) +
  geom_vline(xintercept = mean(pred_vs_act$V2), linewidth = 1, linetype = "dotted", color = "red") + 
  geom_hline(yintercept = mean(pred_vs_act$V3), linewidth = 1, linetype = "dotted", color = "red") + 
  geom_smooth(method = "lm", color = "#9B951B", linewidth = 2) +
  #geom_smooth(method = "loess", color = "#B2F2FD", linewidth = 2) +
  theme_classic() +
  labs(x = "log10(Actual fixation time, generations)", 
       y = "log10(Predicted fixation time, generations)",
       title = paste("\u03c1 = ", cor_est, ", 95 % CI = [",  cor_lwb, ", ", cor_upb, "] , p = ", cor_pvalue, sep = ""))

ggsave(paste("../results/actual_vs_predicted_fix_time_", today, ".png", sep = ""), height = 7, width = 7)

# Measure error between actual and predicted
pred_vs_act$error = (pred_vs_act$V2 - pred_vs_act$V3)^2
names(pred_vs_act)[1] = "ID"
errors = merge(params, pred_vs_act[,c(1,4)], by = "ID")

# How does error correlate with simulation parameters
error_mod = lm(log10(error) ~ N + log10(sweepS) + sigma + h + log10(mu) + log10(R) + tau + f0 + f1 + B + U, data = errors)
summary(error_mod)
plot(error_mod)

error_mod = lm(log10(error) ~ log10(sweepS), data = errors)
summary(error_mod)
```

# How does time to fixation in simulations vary with the simulation parameters?
```{r}
#fix_times_files = list.files(path = "../workflow/data/fix_times", full.names = T)
fix_times_files = list.files(path = "/mnt/scratch/robe1195/Josephs_Lab_Projects/selection-demography-cnn/workflow/data/fix_times", full.names = T)
fix_times = lapply(fix_times_files, fread)

#fail_files = list.files(path = "../workflow/data/fails", full.names = T)
fail_files = list.files(path = "/mnt/scratch/robe1195/Josephs_Lab_Projects/selection-demography-cnn/workflow/data/fails", full.names = T)
fail_times = lapply(fail_files, fread)

extract_number = function(x){
  x$V1[1]
}

fix_times = unlist(lapply(fix_times, extract_number))

fail_times = unlist(lapply(fail_times, extract_number))

fix_times = data.frame(
  tf = fix_times,
  ID = gsub(".txt", "", gsub(".*fix_time_", "",fix_times_files))
)

fail_times = data.frame(
  fails = fail_times,
  ID = gsub(".txt", "", gsub(".*fails_", "", fail_files))
)

fix_times = merge(params, fix_times, by = "ID")
fix_times = merge(fix_times, fail_times, by = "ID")

# square selfing rate
fix_times$sigma2 = fix_times$sigma^2

# calculate probability of failure
# For each simulation, what geometric distribution would give the observed number of failures as it's expected value
fix_times$fix_prob = 1/(fix_times$fails + 1)

# exploratory analysis, look at distribution of fix times
ggplot(fix_times, aes(x=tf)) +
  geom_density() +
  theme_classic() +
  labs(x = "fixation times")

ggplot(fix_times, aes(x=fails)) +
  geom_density() +
  theme_classic() +
  labs(x = "failures")

ggplot(fix_times, aes(x=log10(tf))) +
  geom_density() +
  theme_classic() +
  labs(x = "log10(fixation times)")

ggplot(fix_times, aes(x=log10(fails + 1))) +
  geom_density() +
  theme_classic() +
  labs(x = "log10(failures + 1)")

ggplot(fix_times, aes(x=fix_prob)) +
  geom_density() +
  theme_classic() +
  labs(x = "Probability of fixation")

ggplot(fix_times, aes(x=log10(tf), y = log10(fails + 1))) +
  geom_point() +
  geom_smooth() +
  theme_classic() +
  labs(x = "Fixation time (generations)", y = "Failures")

cor.test(fix_times$tf, fix_times$fails, method = "spearman")

# change growth rate (r) to categorical variable
fix_times$rcat = NA
fix_times$rcat[(fix_times$r == 0)] = "constant"
fix_times$rcat[(fix_times$r > 0) & (fix_times$r < 0.5) & (fix_times$N < fix_times$K)] = "growing"
fix_times$rcat[(fix_times$r > 0) & (fix_times$r < 0.5) & (fix_times$N > fix_times$K)] = "shrinking"
fix_times$rcat[(fix_times$r > 2) & (fix_times$r < sqrt(6))] = "2-cycle"
fix_times$rcat[(fix_times$r > sqrt(6))] = "chaos"

table(fix_times$rcat)

fix_times$rcat = relevel(as.factor(fix_times$rcat), ref = "constant")

# try linear models, see if there are any problems
tf_mod = lm(log10(tf) ~ log10(sweepS) + h + log10(mu) + log10(R) + tau + sigma + sigma2 + sigma*h + sigma*log10(R) + f0 + f1 + N + n + lambda + n*lambda + rcat, data = fix_times)
summary(tf_mod)
plot(tf_mod)

fail_mod = lm(log10(fails + 1) ~ log10(sweepS) + h + log10(mu) + log10(R) + tau + sigma + sigma2 + sigma*h + f0 + f1 + N + n + rcat, data = fix_times)
summary(fail_mod)
plot(fail_mod)

# try glm instead of lm for fixation time
#tf_mod = glm(tf ~ log10(sweepS)+ h + log10(mu) + log10(R) + tau + f0 + f1 + N + lambda + n + r + sigma + sigma2 + sigma*h, data = fix_times, family = Gamma(link = "log"))
#summary(tf_mod)
#plot(tf_mod)

# fit model for failures
fail_mod = glm(fails ~ log10(sweepS)+ h + log10(mu) + log10(R) + tau + sigma + sigma2 + sigma*h + sigma*log10(R) + f0 + f1 + N + lambda + n + n*lambda + rcat, data = fix_times, family = "quasipoisson")
summary(fail_mod)
plot(fail_mod)

# find model with fewest terms but still lots of power
step(tf_mod, direction = "both")

# parametric plot of fixation time versus other parameters
ggplot(fix_times, aes(sigma, h, color = log(tf))) +
  geom_point() +
  theme_classic()

# re-fit model for only recessive mutations
tf_mod = lm(log10(tf) ~ log10(sweepS) + h + log10(mu) + log10(R) + sigma + sigma2 + sigma*h + sigma*log10(R) + N + n, data = fix_times[(fix_times$h < 0.5 & fix_times$f0 == 0 & fix_times$f1 == 1 & fix_times$rcat == "constant" & fix_times$M < 1),])
summary(tf_mod)

cor.test(fix_times$tf[(fix_times$h < 0.5)], fix_times$sigma[(fix_times$h < 0.5)])
```

# Stratified sampling along time to fixation
```{r}

# split
widths_to_try = seq(from = 0.05, to = 1, by = 0.05)
count_thresh = 1000

for(width in widths_to_try){
  breaks = seq(from = min(log10(fix_times$tf)), to = max(log10(fix_times$tf)), by = width)
  fix_times$bin = cut(log10(fix_times$tf), breaks = breaks)
  kept_bins = table(fix_times$bin)[table(fix_times$bin) > count_thresh]
  total_in_downsample = min(kept_bins)*length(kept_bins)
  print(paste("width: ", width, "; total: ", total_in_downsample))
}

breaks = seq(from = min(log10(fix_times$tf)), to = max(log10(fix_times$tf)), by = 0.05)
fix_times$bin = cut(log10(fix_times$tf), breaks = breaks)

kept_bins = table(fix_times$bin)[table(fix_times$bin) > 800]

bin_height = min(kept_bins)

strat_sample = NULL
for(kept_bin in names(kept_bins)){
  fix_times_bin = fix_times[(fix_times$bin == kept_bin),]
  down_sampled_bin = fix_times_bin[sample(1:nrow(fix_times_bin), replace = F, size = bin_height),]
  strat_sample = rbind(strat_sample, down_sampled_bin)
}

# remove weird NAs, not sure where they come from
strat_sample = strat_sample[complete.cases(strat_sample),]
 
# check that tf is now uniformly distributed across data
summary(strat_sample$tf)

ggplot(strat_sample, aes(x = log10(tf))) +
  geom_density() +
  theme_classic() +
  labs(x = "log10(Fixation time, generations)")

ggplot(strat_sample, aes(x = tf)) +
  geom_density() +
  theme_classic() +
  labs(x = "Fixation time, generations")

# randomly subsample data into training, testing, and validation
train_n = round(nrow(strat_sample)*0.8)
test_n = round(nrow(strat_sample)*0.1)
val_n = round(nrow(strat_sample)*0.1)

train_n + test_n + val_n == nrow(strat_sample) # should be TRUE

split_column = c(rep("train", times = train_n), rep("test", times = test_n), rep("val", times = val_n))

strat_sample$split = sample(split_column, replace = F, size = nrow(strat_sample))

table(strat_sample$split)

# What mean squared error would you expect if model predicted the mean for every image?
# This naive model would result in a loss equal to the variance of the response
var(log10(strat_sample$tf))

# save stratified sample
write.table(strat_sample, "stratified_sample.tsv", sep = "\t", quote = F, row.names = F)

```

#
```{r}
expected_seg_sites = function(N,mu,sigma,L){

4*mu*(N/(1 + sigma/(2-sigma)))*L

}

expected_seg_sites(1000,1e-8,0,1e6)
```

#
```{r}
s = runif(6000)
h = runif(6000)

fixtimes = data.frame(
  id = 1:6000,
  hs = h*s
)

fixtimes$bins = cut(fixtimes$hs, breaks = 10)

target = min(table(fixtimes$bins))

for(bin in unique(fixtimes$bins)){
    fixsub = fixtimes[(fixtimes$bins == bin),]
    fixsub[(fixsub$id %in% sample(fixsub$id, size = target, replace = F)),]
}

table(bins)


```

# shape data into one matrix
```{r}
# get max number of variants so, we know what to pad to
rowPadMax = max(unlist(lapply(vcfs, nrow)))

# zero pad or subsample data as necessary so all tables have same dimmensions
zeropad = function(x, rowPadMax){
  varCount = nrow(x)

  # subsample data if there are too many variants
  if(varCount > rowPadMax){
    x = x[sort(sample(1:varCount, rowPadMax, replace = F)),]
  }

  # Add zero-padds if there are too few variants
  if(varCount < rowPadMax){
    lastVar = x[varCount,"POS"]
    for(i in 1:(rowPadMax - varCount)){
      x = rbind(x, c(1, lastVar + i, "MT=0;", rep(0, times = 128)))
    }
  }
  
  return(x)
}

vcfs = lapply(vcfs, zeropad, rowPadMax = rowPadMax)

# melt vcfs
meltvcfs = function(x){
  x = melt(x, id.vars = c("CHROM", "POS", "INFO"))
  return(x[,5])
}

vcfs = lapply(vcfs, meltvcfs)

# combine all vcfs into one matrix
vcfs = do.call("cbind", vcfs)
dim(vcfs)

# convert all columns to numeric
vcfs = apply(vcfs, MARGIN = 2, as.numeric)
dim(vcfs)
```

# PCA and correlations
```{r}
# simulations should be rows
tvcfs = t(vcfs)

# tvcfsNoZero = tvcfs[,apply(tvcfs, 2, function(x){length(unique(x)) > 1})]

# do PCA
vcfpca = prcomp(tvcfs, center = T)

plot(vcfpca)
plot(vcfpca$x)

# If outcome is binary, look for association using logistic regression
regframe = as.data.frame(cbind(y, vcfpca$x))
regframe$y = as.factor(regframe$y)

mod = glm(y ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, data = regframe, family = "binomial")
summary(mod)

# visually show how PCs correlate with dependent variable
ggplot(regframe, aes(PC1, PC2, color = y)) +
  geom_point() +
  theme_classic() +
  scale_color_scico_d(palette = "hawaii", labels = c("Neutral", "Selective sweep"), name = "Simulation")

ggsave("pca.png", scale = 0.5, width = 10, height = 7)

# correlate PCs with the dependent variable
plot(vcfpca$x[,1], y)
cor.test(vcfpca$x[,1], y, method = "spearman")

plot(vcfpca$x[,2], log(y))
cor.test(vcfpca$x[,2], log(y), method = "spearman")
```

